{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: A Primer On Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post of this series, I wrote about how pre-processing is the first step we should take when analysing text data. This is done so that we may maximise the performances of our algorithms based on the task we want to accomplish. Specifically, we want to filter out any aspect of the data that will at worst hinder and at best not be helpful in achieving our task.\n",
    "\n",
    "The following is non-exhaustive roadmap that I consulted in my sentiment analysis project:\n",
    "\n",
    "- Remove stopwords (context dependent)\n",
    "- Stemming and lemmatisation\n",
    "- Remove hashtags, mentions and emojis with text they represent (for Twitter)\n",
    "- Replace contractions with their full forms\n",
    "- Remove punctuations\n",
    "- Convert everything to lowercase\n",
    "- Remove HTML tags if present\n",
    "\n",
    "Let's take a look, with code, how these steps can be executed. To better frame this analysis, we will be using Amazon Product Data for their videogames category by Julian McAuley (*Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016, found at http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz*)\n",
    "\n",
    "This data contains the following useful variables:\n",
    "\n",
    "- body of the review\n",
    "- a 'summary' or review title\n",
    "- an overall product rating out of\n",
    "- the reviewer's username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Removing stopwords (or rather, should we remove stopwords?) using spaCy\n",
    "\n",
    "Words that do not hold a lot of semantic connotation, and thus are not useful for linking input (text data) to output (semantic label) are considered stopwords. These include 'a', 'the', 'who', 'why', pronouns and so on. \n",
    "\n",
    "However, we must be very careful in deciding whether or not to pursue this step, especially in the context of sentiment analysis. This is because removing stopwords may subtract from the true meaning of a sentence and thus alter it considerably. Consider the word 'not'. \"I do not like this colour\" has the opposite sentiment of \"I like this colour\". So if we remove the word 'not' from a review, we have effectively reversed the sentiment of the view. \n",
    "\n",
    "As a result, in the context of sentiment analysis, it is advised to not execute this step. I will still display how to remove stopwords as a matter of demonstration. We can use a very useful NLP module called spaCy to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2HD75EMZR8QLN</td>\n",
       "      <td>123</td>\n",
       "      <td>[8, 12]</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pay to unlock content? I don't think so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A3UR8NLLY1ZHCX</td>\n",
       "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Good rally game</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                        reviewerName  helpful  \\\n",
       "0  A2HD75EMZR8QLN                                 123  [8, 12]   \n",
       "1  A3UR8NLLY1ZHCX  Alejandro Henao \"Electronic Junky\"   [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Installing the game was a struggle (because of...        1   \n",
       "1  If you like rally cars get this game you will ...        4   \n",
       "\n",
       "                                    summary  \n",
       "0  Pay to unlock content? I don't think so.  \n",
       "1                           Good rally game  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data provided is in JSON format, so we can load it in a pandas dataframe using the following code:\n",
    "import pandas as pd\n",
    "data = pd.read_json('/Users/alitaimurshabbir/Desktop/reviews_Video_Games_5.json', lines=True)\n",
    "data.drop(['asin','unixReviewTime', 'reviewTime'], axis = 1, inplace = True) #drop unnecessary columns\n",
    "data.head(2) #preview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy comes with its own defined set of 312 stopwords. We can use this predefined list to remove stopwords from our reviews. It is useful to know that we can add our own stopwords to such a set (https://spacy.io/usage/adding-languages/#stop-words) so let's go ahead and import spaCy and execute this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['via', '’d', 'whatever', 'after', 'latter', 'its', 'seems', 'what', 'by', '‘d']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en') #load the English language model of spaCy\n",
    "stopWords = spacy.lang.en.STOP_WORDS #load stopwords\n",
    "\n",
    "#to see a few examples of these stopwords, I can convert the first 10 elements of this set into a list\n",
    "\n",
    "stopList = list(stopWords)[:10]\n",
    "print(stopList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords from our reviews using list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviewTextNoStopwords'] = data['reviewText'].apply(lambda x:' '.join([word for word in x.split() if word not in stopWords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that we use the methods *join* and *split*. \n",
    "\n",
    "We use them because pre-processing has to be performed on items in a list, not on a string. So *split()* is used to accomplish this. Subsequently, *join()*, is used to turn back the split components into a sentence/string, once a pre-processing step has been performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comparison of the review text pre- and post-removal of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextNoStopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>Installing game struggle (because games window...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>If like rally cars game fun.It oriented &amp;#34;E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1st shipment received a book instead of the ga...</td>\n",
       "      <td>1st shipment received book instead game.2nd sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Installing the game was a struggle (because of...   \n",
       "1  If you like rally cars get this game you will ...   \n",
       "2  1st shipment received a book instead of the ga...   \n",
       "\n",
       "                               reviewTextNoStopwords  \n",
       "0  Installing game struggle (because games window...  \n",
       "1  If like rally cars game fun.It oriented &#34;E...  \n",
       "2  1st shipment received book instead game.2nd sh...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[0, 1, 2], ['reviewText', 'reviewTextNoStopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Lemmatisation using NLTK\n",
    "\n",
    "Lemmatisation is the process of reducing inflectional forms and sometimes derivationally related forms of a word to a common base form. The lemma of 'walking', for example, is 'walk'. As with removing stopwords, lemmatisation is intended to improve model performance, although it is again possible that performance actually declines instead. This is because both of the aforementioned steps are intended to improve the *recall* metric and it tends to negatively impact *precision*. As a result, using either technique depends on the metrics one is focused on.\n",
    "\n",
    "As with stopword removal, I will demonstarte how lemmatisation is achieved. We can use another very useful NLP module, called **NLTK**, and its function, WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviewText'] = data['reviewText'].apply(lambda x:' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Removing hashtags and mentions (Twitter-specific) using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things about NLP is that sources of data can be very specific in terms of the quirks they have. Sometimes we must deal with those quirks and sometimes we can leave them be. Data from Twitter, for example, will contain hashtags (#) and mentions (@username) because those mechanics are inherent to the platform. They are also not particularly useful in analysis, so we can remove them.\n",
    "\n",
    "Doing so requires familiarty with yet another great module named **Regex**, or Regular Expression. Regex has special considerations for social media regular expressions and for Twitter in particular\n",
    "\n",
    "Two such expressions in which we are interested are:\n",
    "\n",
    "- @[A-Za-z0-9]+ which represents all kinds of mentions\n",
    "- #[A-Za-z0-9]+ which represents all kinds of hashtags\n",
    "\n",
    "Since our Amazon data doesn't have many hashtags or mentions, let's create a single string containing these elements and witness how Regex works. We will be using the *join*, *sub* and *split* string methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "someText = '''Inter Milan goalkeeper @Samir Handanovic will not be #going to PSG. his agent @massimo venturella said \n",
    "to football italia: \"I can confirm that there were negotiations with PSG, which we have broken off. PSG is not an\n",
    "option. Real Madrid and Liverpool are the other strong rumours. #inter #lfc'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "someText = ' '.join(re.sub( \"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", ' ', someText).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter Milan goalkeeper Handanovic will not be to PSG. his agent venturella said to football italia: \"I can confirm that there were negotiations with PSG, which we have broken off. PSG is not an option. Real Madrid and Liverpool are the other strong rumours.\n"
     ]
    }
   ],
   "source": [
    "print(someText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Remove URLs using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove URLs in the same way we removed mentions and hashtags. This time the expression needed is **\\w+:\\/\\/\\S+!**, which represents all URLs matching with http:// or https://"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Remove punctuations using Regex\n",
    "\n",
    "As before, Regex makes this very easy. We use the expression **.,!?:;-=** to represent punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"] = data['reviewText'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Remove HTML tags using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very simple and self-explanatory. Our someText variable above does not have HTML tags, so the following shows a general way to accomplish this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textVariable = BeautifulSoup(textVariable).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Lower-case all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviewText'] = data['reviewText'].apply(lambda x: ' '.join(word.lower() for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, all text can been converted to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         installing the game wa a struggle (because of ...\n",
       "1         if you like rally car get this game you will h...\n",
       "2         1st shipment received a book instead of the ga...\n",
       "3         i got this version instead of the ps3 version,...\n",
       "4         i had dirt 2 on xbox 360 and it wa an okay gam...\n",
       "                                ...                        \n",
       "231775    funny people on here are rating seller that ar...\n",
       "231776    all this is is the deluxe 32gb wii u with mari...\n",
       "231777    the package should have more red on it and sho...\n",
       "231778    can get this at newegg for $329.00 and the pac...\n",
       "231779    this is not real, you can go to any retail sto...\n",
       "Name: reviewText, Length: 231780, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Replace contractions with their full forms\n",
    "\n",
    "A contraction is the shortened form of a common phrase. \"Isn't\" is the contraction of \"is not\", and there may be some performance benefits to be gained by expanding such contractions. However, this is sligthly trickier than the previous few steps as no pre-defined regular expression exists for this purpose.\n",
    "\n",
    "One solution to this is creating our own dictionary with the keys-value pairs representing the contracted and expanded forms, respectively, of phrases. Here's a short example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "textVariable = 'Sean isn\\'t in the barn; we\\'ve checked it already'\n",
    "\n",
    "contractions = {\"isn't\":\"is not\", \"we've\":\"we have\"}\n",
    "textVariable  = textVariable.replace(\"’\",\"'\")\n",
    "words = textVariable.split()\n",
    "reformed = [contractions[word] if word in contractions else word for word in words]\n",
    "textVariable = \" \".join(reformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing this out shows us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sean is not in the barn; we have checked it already'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textVariable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've quickly seen some of the more common substeps within pre-processing of text data and how to execute them. In the next post, we will explore the what, how and why of feature representation of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
